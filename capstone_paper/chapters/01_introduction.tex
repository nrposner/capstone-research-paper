% Chapter: Introduction
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}
\section{Introduction}

\subsection{Problem Statement}

Drone light shows represent a unique synthesis of creative design and algorithmic precision, transforming abstract artistic concepts into tightly coordinated aerial performances. While their visual and cultural impact has grown rapidly, the process of designing such shows remains complex, highly specialized, and constrained by strict physical and regulatory boundaries. Translating human imagination into executable drone trajectories requires both aesthetic sensitivity and formal guarantees of safety, spacing, and feasibility.

Recent proposals have explored using Large Language Models (LLMs) to bridge the gap between natural language expression and swarm behavior generation. In principle, this approach offers an intuitive design interface: a user describes the desired effect --- “form a spiral that blossoms into a sphere” --- and an LLM translates that prompt into flight paths or formation parameters. However, existing systems such as CLIPSwarm, SwarmGPT, and LLM-Flock have largely remained proof-of-concept demonstrations. These architectures use language models as semantic translators, producing intermediate representations such as geometric outlines or waypoints, which are then refined through separate analytic solvers. As a result, the LLM occupies a peripheral role: it initiates the design process but does not ensure executable, safe, or optimized outcomes.

The practical consequence of this separation is that current research prototypes have not achieved the level of reliability or workflow integration required for real-world deployment. The artistic and semantic potential of language-based co-design remains underutilized, while the technical constraints that define feasible drone motion are handled independently through manual or traditional optimization methods. This disconnect mirrors broader challenges in LLM-assisted content generation, where models excel at producing semantically rich ideas but struggle when constrained by formal syntactic or physical rules.

Our work begins from this recognition and seeks to explore whether these two modalities — the semantic flexibility of language models and the syntactic precision of analytical systems — can be combined more effectively. Specifically, we propose an integrated pipeline in which the LLM functions not as a direct controller but as a generative front-end for a structured, verifiable production system. The final stages of this pipeline, including trajectory validation and compilation into deployable show files, are handled through the \textbf{Skybrush Studio API}, a professional toolchain for drone show design and execution. This framework grounds the generative capabilities of LLMs within the constraints of industry-standard safety and performance requirements.

\subsection{Analysis Goals}

The central objective of this research is to evaluate how language-based systems can meaningfully contribute to the workflow of drone show design, rather than to demonstrate isolated technical feasibility. We aim to examine how semantic generative processes can be coupled with formal analytic stages to produce syntactically valid and operationally safe results.

Accordingly, we define the following guiding questions:

\begin{enumerate}
  \item What are the real creative, technical, and operational constraints that define professional drone show design?
  \item How can the expressive power of LLMs be harnessed to produce meaningful inputs for downstream analytical solvers and simulation systems?
  \item Can we formalize a pipeline in which semantic generation and syntactic enforcement coexist — preserving creative flexibility while ensuring feasibility?
  \item What role can established production systems such as Skybrush Studio play in this integration, and how might such a hybrid approach inform future design interfaces?
\end{enumerate}

This investigation treats LLMs not as autonomous choreographers but as components within a larger cybernetic structure. By analyzing their performance in tandem with analytical solvers, we seek to clarify their true contribution to the creative process and identify where the boundary between generative flexibility and formal verification should lie.

\subsection{Scope}

This project focuses on the design phase of drone light show creation, emphasizing the transition from conceptual imagery or verbal description to a validated, executable formation plan. It does not involve real-time flight control, deployment logistics, or novel control theory. Rather, it seeks to bridge the semantic and syntactic stages of the production pipeline.

The scope of this work includes:
\begin{itemize}
  \item Developing and testing a modular pipeline that combines generative LLM outputs with analytic transformation and validation stages.
  \item Implementing 2D and 3D formation generation from images and meshes using sampling algorithms inspired by computer graphics.
  \item Integrating the final stage of compilation and verification through the Skybrush Studio API, enabling compatibility with professional-grade drone control systems.
  \item Exploring the potential of Model Context Protocol (MCP)-style interaction to provide real-time, human-in-the-loop editing within design environments such as Blender.
\end{itemize}

We do not aim to develop a self-contained generative model for drone trajectories, nor to produce a fully autonomous end-to-end system. The purpose of this work is to formalize and test a \textit{structured interface} between generative language systems and the analytic frameworks that ensure physical validity. In doing so, it provides both a practical and conceptual foundation for future tools that support co-creative, interpretable, and safe design of drone swarm performances.

